{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c52bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# === Load API keys ===\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"‚ùå Please set GEMINI_API_KEY in your .env file.\")\n",
    "    exit()\n",
    "\n",
    "# --- Load and chunk PDF ---\n",
    "def load_constitution(pdf_path):\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        return splitter.split_documents(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Build vectorstore ---\n",
    "def build_vectorstore(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore.save_local(\"database\")\n",
    "    return vectorstore\n",
    "\n",
    "# --- Tavily search tool ---\n",
    "@tool\n",
    "def tavily_search(query: str):\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"api_key\": TAVILY_API_KEY, \"query\": query, \"max_results\": 3}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        results = response.json().get(\"results\", [])\n",
    "        return \"\\n\".join([f\"{r['title']}: {r['content']}\" for r in results]) if results else \"No info found.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# --- Agent Executor ---\n",
    "def get_agent_executor():\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", google_api_key=GEMINI_API_KEY)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    try:\n",
    "        vectorstore = FAISS.load_local(\"database\", embeddings, allow_dangerous_deserialization=True)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading vectorstore: {e}\")\n",
    "        return None\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type=\"stuff\",\n",
    "        input_key=\"query\"\n",
    "    )\n",
    "\n",
    "    @tool\n",
    "    def pdf_qa(query: str):\n",
    "        return qa_chain.invoke({\"query\": query}).get(\"result\", \"\")\n",
    "\n",
    "    tools = [pdf_qa, tavily_search]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant. Use pdf_qa first; then tavily_search if needed.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "    return AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# --- Main Program ---\n",
    "def main():\n",
    "    print(\"üá≥üá¨ Nigerian Constitution Chatbot (type 'exit' to quit)\\n\")\n",
    "    \n",
    "    PDF_PATH = \"constitution.pdf\"\n",
    "    chunks = load_constitution(PDF_PATH)\n",
    "    if not chunks:\n",
    "        print(\"‚ùå Could not load PDF or no chunks found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    build_vectorstore(chunks)\n",
    "    agent_executor = get_agent_executor()\n",
    "    if not agent_executor:\n",
    "        return\n",
    "\n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nYou: \").strip()\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"\\n‚úÖ Chat ended. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Convert stored chat history to LangChain messages\n",
    "        langchain_history = []\n",
    "        for u_msg, a_msg in chat_history:\n",
    "            langchain_history.append(HumanMessage(content=u_msg))\n",
    "            langchain_history.append(AIMessage(content=a_msg))\n",
    "\n",
    "        try:\n",
    "            response = agent_executor.invoke({\"input\": query, \"chat_history\": langchain_history})\n",
    "            answer = response.get(\"output\", \"\")\n",
    "            print(f\"Assistant: {answer}\")\n",
    "            chat_history.append((query, answer))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# --- Run ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
